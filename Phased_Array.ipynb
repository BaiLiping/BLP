{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaiLiping/BLP/blob/master/Phased_Array.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l56SjgRIo2iB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "wave_length=0.7\n",
        "phase=180/5\n",
        "\n",
        "def generate_target():\n",
        "    index=np.random.choice(range(48))\n",
        "    position=np.unravel_index(index,(8,6))\n",
        "    return position\n",
        "    \n",
        "def location_info(target_location):\n",
        "    distance=math.sqrt(pow((3*(target_location[0])),2)+pow(3*(target_location[1]),2))\n",
        "    impact=(1/(distance+1))*30*math.cos(distance/wave_length)\n",
        "    return impact\n",
        "        \n",
        "def compute_reward(x,y,p,target_location):\n",
        "    distance=math.sqrt(pow((3*(x-target_location[0])),2)+pow(3*(y-target_location[1]),2))\n",
        "    reward=(1/(distance+1))*30*math.cos(distance/wave_length+(p-1)*phase)\n",
        "    return reward\n",
        "\n",
        "class PhasedArrayEnv(object):\n",
        "    def __init__(self):\n",
        "        self.wave_length=0.7\n",
        "        self.phase=180/5\n",
        "        self.nA=47*6\n",
        "        self.state=np.zeros(48)\n",
        "        self.target_location=generate_target()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.target_location=generate_target()\n",
        "        self.state=np.zeros(48)\n",
        "        self.state[0]=location_info(self.target_location)\n",
        "        return self.state\n",
        "    \n",
        "    def get_nA(self):\n",
        "        return self.nA\n",
        "\n",
        "    def step(self,action):\n",
        "        x=np.unravel_index(action+5,(8,6,6))[0]\n",
        "        y=np.unravel_index(action+5,(8,6,6))[1]\n",
        "        p=np.unravel_index(action+5,(8,6,6))[2]+1\n",
        "        state_index=np.ravel_multi_index((x,y),(8,6))\n",
        "        self.state[state_index]=p\n",
        "        reward=compute_reward(x,y,p,self.target_location)\n",
        "        return self.state,reward\n",
        "    \n",
        "    def render(self):\n",
        "        outfile=sys.stdout\n",
        "        for i in range(48):\n",
        "            position=np.unravel_index(i,(8,6))\n",
        "            if self.state[i]!=0:\n",
        "                output='  '\n",
        "                output+=str(int(self.state[i]))\n",
        "                output+='  '\n",
        "            else:\n",
        "                output='  _  '\n",
        "    \n",
        "            if position[1]==0:\n",
        "                output=output.lstrip()\n",
        "            if position[1]==(8,6)[1]-1:\n",
        "                output=output.rstrip()\n",
        "                output+='\\n'\n",
        "            outfile.write(output)\n",
        "        outfile.write('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUPRPnWQpDPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "01fc8352-c015-4506-ec8c-24f1a386e4d8"
      },
      "source": [
        "env=PhasedArrayEnv()\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-3    _    _    _    _    _\n",
            "_    _    _    _    _    _\n",
            "_    _    _    _    _    _\n",
            "_    _    _    _    _    _\n",
            "_    _    _    _    _    _\n",
            "_    _    _    _    _    _\n",
            "_    _    _    _    _    _\n",
            "_    _    _    _    _    _\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t2fmngio87s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dec678c8-6bdc-4df9-cb2e-97b3f22dcb06"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Dense, Activation, Input\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "class Agent(object):\n",
        "    def __init__(self, alpha, beta, gamma, nA):\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.input_dims = 48\n",
        "        self.fc1_dims = 48\n",
        "        self.fc2_dims = 48\n",
        "        self.nA = nA\n",
        "\n",
        "        self.actor, self.policy = self.policy_estimator()\n",
        "        self.critic=self.value_estimator()\n",
        "\n",
        "    def policy_estimator(self):\n",
        "        data = Input(shape=(self.input_dims,))\n",
        "        td_error = Input(shape=[1])\n",
        "        dense1 = Dense(self.fc1_dims, activation='relu')(data)#to equalize\n",
        "        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n",
        "        probs = Dense(self.nA, activation='softmax')(dense2)\n",
        "        \n",
        "        #most likely this is wrong\n",
        "        def custom_loss(y_true, y_pred):\n",
        "            y_pred_trimmed = K.clip(y_pred, 1e-8, 1-1e-8)\n",
        "            score_function = K.log(y_pred_trimmed)\n",
        "            return K.sum(score_function*td_error)\n",
        "\n",
        "        actor = Model(input=[data, td_error], output=[probs])\n",
        "        actor.compile(optimizer=Adam(lr=self.alpha), loss=custom_loss)\n",
        "        distribution = Model(input=[data], output=[probs])\n",
        "\n",
        "        return actor,distribution\n",
        "    \n",
        "    def value_estimator(self):\n",
        "        data=Input(shape=(self.input_dims,))\n",
        "        dense1=Dense(self.fc1_dims,activation='relu')(data)\n",
        "        dense2=Dense(self.fc2_dims,activation='relu')(dense1)\n",
        "        output=Dense(1, activation='linear')(dense2)\n",
        "        critic=Model(input=[data], output=[output])\n",
        "        critic.compile(optimizer=Adam(lr=self.beta), loss='mean_squared_error')\n",
        "        return critic\n",
        "    def get_value(self,state):\n",
        "        state = state[np.newaxis,:]\n",
        "        value=self.critic.predict(state)[0][0]\n",
        "        return value\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        state = state[np.newaxis,:]/6\n",
        "        probabilities = self.policy.predict(state)\n",
        "        action = np.random.choice(np.arange(self.nA), p=probabilities[0])\n",
        "        while True:\n",
        "            if action!=0:\n",
        "                print(probabilities[0])\n",
        "                return action\n",
        "            else:\n",
        "                action=np.random.choice(np.arange(self.nA),p=probabilities[0])\n",
        "\n",
        "    def learn(self, state, action, reward, new_state):\n",
        "        state = state[np.newaxis,:]/6\n",
        "        new_state=new_state[np.newaxis,:]/6\n",
        "        new_critic_value = self.critic.predict(new_state)\n",
        "        critic_value = self.critic.predict(state)\n",
        "        \n",
        "        actions = np.zeros([1, self.nA])\n",
        "        actions[np.arange(1), action] = 1\n",
        "\n",
        "        \n",
        "        td_target = reward + self.gamma*new_critic_value\n",
        "        td_error =  td_target - critic_value\n",
        "\n",
        "        self.actor.fit([state, td_error], actions)\n",
        "        self.critic.fit(state, td_target)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}